{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IndoRE-datathon-2021.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOpaWy/RzV69XDHHsAQzFu3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fE_EQEhFRUPc","executionInfo":{"status":"ok","timestamp":1638795365976,"user_tz":-330,"elapsed":1215,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"b7b924f5-8691-438d-9999-3550145d2e49"},"source":["# ! pip install kaggle\n","import pandas as pd\n","from google.colab import drive\n","from torch.utils.tensorboard import SummaryWriter\n","drive.mount('/content/gdrive/')\n","\n","%cd /content/gdrive/My Drive/Kaggle\n","# writer = SummaryWriter('')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","/content/gdrive/My Drive/Kaggle\n"]}]},{"cell_type":"code","metadata":{"id":"PbaxMJYs4QNl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638536641704,"user_tz":-330,"elapsed":10492,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"f574e8a5-07ad-4711-b13d-64a01399ef82"},"source":["import random\n","!pip install transformers\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertModel, BertPreTrainedModel\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm, trange\n","from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 7.2 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 671 kB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 47.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 46.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 48.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"]}]},{"cell_type":"code","metadata":{"id":"xMLuUH7Q4kvi"},"source":["def set_seed():\n","    random.seed(77)\n","    np.random.seed(77)\n","    torch.manual_seed(77)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(77)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wH-Lw9gPTrJI"},"source":["#! unzip valid.tsv.zip\n","#! unzip train.tsv.zip\n","df = pd.read_csv('train.tsv', sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"EEthhjRsVSmr","executionInfo":{"status":"ok","timestamp":1638536644585,"user_tz":-330,"elapsed":1559,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"f4109e84-dbfb-4c55-82de-871ed9348707"},"source":["df_en = pd.read_csv('en.tsv', sep = '\\t')\n","df_en"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Relation</th>\n","      <th>Sentence</th>\n","      <th>NER1</th>\n","      <th>NER2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>director</td>\n","      <td>&lt;e1&gt;The Battle of the Sexes&lt;/e1&gt; is a 1928 Ame...</td>\n","      <td>WORK_OF_ART</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>director</td>\n","      <td>&lt;e1&gt;The Day of Faith&lt;/e1&gt; is a 1923 American s...</td>\n","      <td>WORK_OF_ART</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>director</td>\n","      <td>&lt;e1&gt;The Straight Story&lt;/e1&gt; is a 1999 biograph...</td>\n","      <td>WORK_OF_ART</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>director</td>\n","      <td>Baltz starred in \"&lt;e2&gt;Paul Thomas Anderson&lt;/e2...</td>\n","      <td>WORK_OF_ART</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>director</td>\n","      <td>He also appeared in &lt;e2&gt;Clint Eastwood&lt;/e2&gt;'s ...</td>\n","      <td>WORK_OF_ART</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3014</th>\n","      <td>capital</td>\n","      <td>He is a native of the town of &lt;e2&gt;Malolos&lt;/e2&gt;...</td>\n","      <td>GPE</td>\n","      <td>GPE</td>\n","    </tr>\n","    <tr>\n","      <th>3015</th>\n","      <td>capital</td>\n","      <td>&lt;e2&gt;Moscow&lt;/e2&gt; is the capital of &lt;e1&gt;Russia&lt;/...</td>\n","      <td>GPE</td>\n","      <td>GPE</td>\n","    </tr>\n","    <tr>\n","      <th>3016</th>\n","      <td>capital</td>\n","      <td>&lt;e2&gt;Seoul&lt;/e2&gt; was the capital of various Kore...</td>\n","      <td>GPE</td>\n","      <td>GPE</td>\n","    </tr>\n","    <tr>\n","      <th>3017</th>\n","      <td>capital</td>\n","      <td>The headquarters of the institution are locate...</td>\n","      <td>GPE</td>\n","      <td>GPE</td>\n","    </tr>\n","    <tr>\n","      <th>3018</th>\n","      <td>capital</td>\n","      <td>&lt;e2&gt;St. John's&lt;/e2&gt; served as the capital city...</td>\n","      <td>GPE</td>\n","      <td>GPE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3019 rows × 4 columns</p>\n","</div>"],"text/plain":["      Relation  ...    NER2\n","0     director  ...  PERSON\n","1     director  ...  PERSON\n","2     director  ...  PERSON\n","3     director  ...  PERSON\n","4     director  ...  PERSON\n","...        ...  ...     ...\n","3014   capital  ...     GPE\n","3015   capital  ...     GPE\n","3016   capital  ...     GPE\n","3017   capital  ...     GPE\n","3018   capital  ...     GPE\n","\n","[3019 rows x 4 columns]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"yZrjgjyVVuQp","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1638794433620,"user_tz":-330,"elapsed":215388,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"614ce152-b5e9-4bba-fc16-e4caa2292167"},"source":["!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install inltk"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.3.1+cpu\n","  Downloading https://download.pytorch.org/whl/cpu/torch-1.3.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (111.8 MB)\n","\u001b[K     |████████████████████████████████| 111.8 MB 80 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.3.1+cpu) (1.19.5)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.10.0+cu111\n","    Uninstalling torch-1.10.0+cu111:\n","      Successfully uninstalled torch-1.10.0+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.3.1+cpu which is incompatible.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.3.1+cpu which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.3.1+cpu which is incompatible.\u001b[0m\n","Successfully installed torch-1.3.1+cpu\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting inltk\n","  Downloading inltk-0.9-py3-none-any.whl (13 kB)\n","Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.7/dist-packages (from inltk) (2.2.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from inltk) (1.4.1)\n","Collecting typing\n","  Downloading typing-3.7.4.3.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 3.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from inltk) (1.19.5)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from inltk) (2.7.3)\n","Collecting fastai==1.0.57\n","  Downloading fastai-1.0.57-py3-none-any.whl (233 kB)\n","\u001b[K     |████████████████████████████████| 233 kB 11.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from inltk) (2.23.0)\n","Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.7/dist-packages (from inltk) (1.0.0)\n","Collecting aiohttp>=3.5.4\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 52.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from inltk) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from inltk) (1.1.5)\n","Collecting async-timeout>=3.0.1\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 41.5 MB/s \n","\u001b[?25hRequirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from inltk) (7.352.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from inltk) (3.13)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from inltk) (7.1.2)\n","Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from inltk) (1.3.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from inltk) (3.2.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from inltk) (4.6.3)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.57->inltk) (1.3.1+cpu)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.57->inltk) (0.11.1+cu111)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 42.2 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 55.2 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (3.10.0.2)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 45.1 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (2.0.8)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (21.2.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (3.0.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (2.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (57.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (4.62.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (0.8.2)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (2.10)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (3.0.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->inltk) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->inltk) (2018.9)\n","Collecting torch>=1.0.0\n","  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.3 MB/s eta 0:00:39tcmalloc: large alloc 1147494400 bytes == 0x55b82333a000 @  0x7fce4e542615 0x55b7e8ffd4cc 0x55b7e90dd47a 0x55b7e90002ed 0x55b7e90f1e1d 0x55b7e9073e99 0x55b7e906e9ee 0x55b7e9001bda 0x55b7e9073d00 0x55b7e906e9ee 0x55b7e9001bda 0x55b7e9070737 0x55b7e90f2c66 0x55b7e906fdaf 0x55b7e90f2c66 0x55b7e906fdaf 0x55b7e90f2c66 0x55b7e906fdaf 0x55b7e9002039 0x55b7e9045409 0x55b7e9000c52 0x55b7e9073c25 0x55b7e906e9ee 0x55b7e9001bda 0x55b7e9070737 0x55b7e906e9ee 0x55b7e9001bda 0x55b7e906f915 0x55b7e9001afa 0x55b7e906fc0d 0x55b7e906e9ee\n","\u001b[K     |████████████████████████████████| 881.9 MB 17 kB/s \n","\u001b[?25hBuilding wheels for collected packages: typing\n","  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26324 sha256=17d3f6d1fba38180a3e243ace401edb25fa737494642b132f4579ef0373f1d4a\n","  Stored in directory: /root/.cache/pip/wheels/35/f3/15/01aa6571f0a72ee6ae7b827c1491c37a1f72d686fd22b43b0e\n","Successfully built typing\n","Installing collected packages: torch, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, typing, sentencepiece, fastai, aiohttp, inltk\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.3.1+cpu\n","    Uninstalling torch-1.3.1+cpu:\n","      Successfully uninstalled torch-1.3.1+cpu\n","  Attempting uninstall: fastai\n","    Found existing installation: fastai 1.0.61\n","    Uninstalling fastai-1.0.61:\n","      Successfully uninstalled fastai-1.0.61\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 fastai-1.0.57 frozenlist-1.2.0 inltk-0.9 multidict-5.2.0 sentencepiece-0.1.96 torch-1.10.0 typing-3.7.4.3 yarl-1.7.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","typing"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"TnwDJCgHTezS","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1638795379901,"user_tz":-330,"elapsed":1170,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"5b692b08-80a8-4b08-853f-262610ec0945"},"source":["from inltk.inltk import identify_language, tokenize, reset_language_identifying_models\n","inputs = []\n","\n","reset_language_identifying_models()\n","#identify_language('कहानियों में यह सुना जाता है कि एक रहस्यमय तिब्बती सिद्ध पुरुष और ')\n","\n","for i in range(len(df)):\n","  reset_language_identifying_models()\n","  sentence = str(df['Sentence'].iloc[i])\n","  lang = identify_language(sentence)\n","  sentence = ' '.join(tokenize(sentence, lang))\n","  inputs.append(df['NER1'].iloc[i]+'-'+df['NER2'].iloc[i]+' '+sentence)\n","  break\n","\n","df['CLS_sentence'] = inputs\n","\n","df = df[['CLS_sentence', 'Relation']]\n","print(df)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-92804f69a809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mreset_language_identifying_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#identify_language('कहानियों में यह सुना जाता है कि एक रहस्यमय तिब्बती सिद्ध पुरुष और ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/inltk/inltk.py\u001b[0m in \u001b[0;36mreset_language_identifying_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreset_language_identifying_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mreset_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/inltk/utils.py\u001b[0m in \u001b[0;36mreset_models\u001b[0;34m(folder_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreset_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'models'\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf'{folder_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/inltk/models/all'"]}]},{"cell_type":"code","metadata":{"id":"iwteGyXXiBYa"},"source":["ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n","def load_tokenizer():\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n","    return tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSEFEhQN_ZDq","executionInfo":{"status":"ok","timestamp":1638536644588,"user_tz":-330,"elapsed":30,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"1143317d-6a9e-4506-dc23-90e7a915eefb"},"source":["df['NER1'].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['PERSON', 'EVENT', 'ORG', 'OTHER', 'GPE', 'WORK_OF_ART', 'LOC',\n","       'NORP', 'PRODUCT', 'FAC', 'GE', 'DATE'], dtype=object)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBfZgHvM_nwT","executionInfo":{"status":"ok","timestamp":1638536644589,"user_tz":-330,"elapsed":28,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"07bb0b33-4b5b-4bf8-a772-02f9339911e6"},"source":["df['NER2'].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['OTHER', 'PERSON', 'ORG', 'GPE', 'LOC', 'LANGUAGE', 'NORP',\n","       'WORK_OF_ART', 'EVENT'], dtype=object)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFE_ZYGL_rt4","executionInfo":{"status":"ok","timestamp":1638536644589,"user_tz":-330,"elapsed":26,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"c4defc9a-6821-41ba-e95a-9dfbc72e50c4"},"source":["df['Relation'].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['occupation', 'sport', 'sibling', 'parent_organization', 'mother',\n","       'father', 'place_of_birth', 'winner', 'student_of', 'employer',\n","       'spouse', 'founded_by', 'child', 'capital', 'director',\n","       'tributary', 'subsidiary', 'original_language_of_film_or_TV_show',\n","       'participant', 'award_received', 'place_of_death', 'capital_of',\n","       'position_held', 'discoverer_or_inventor', 'nominated_for'],\n","      dtype=object)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"KPuBm33n_8mn"},"source":["labels = []\n","for i in range(len(df_en)):\n","  labels.append(df_en['NER1'].iloc[i]+'-'+df_en['NER2'].iloc[i])\n","\n","df_en['CLS'] = labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_Xg7qfZBbdu","executionInfo":{"status":"ok","timestamp":1638536644590,"user_tz":-330,"elapsed":23,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"1151e912-1ff9-4bca-9b2e-259831331683"},"source":["df_en['CLS'].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['WORK_OF_ART-PERSON', 'ORG-PERSON', 'PERSON-ORG', 'PERSON-NORP',\n","       'OTHER-PERSON', 'EVENT-GPE', 'PERSON-PERSON', 'ORG-ORG',\n","       'PERSON-OTHER', 'GPE-GPE', 'OTHER-GPE', 'OTHER-OTHER', 'LOC-LOC',\n","       'PERSON-GPE', 'WORK_OF_ART-LANGUAGE'], dtype=object)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"bN_XPNwQUWhf","colab":{"base_uri":"https://localhost:8080/","height":571},"executionInfo":{"status":"error","timestamp":1638537957415,"user_tz":-330,"elapsed":551,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"4e693684-5e74-40e8-8270-6acc36c5f83c"},"source":["labels = list(set(labels))\n","id = {}\n","for i in range(len(labels)):\n","  id[labels[i]] = i\n","\n","lst = []\n","for i in range(len(df)):\n","  lst.append(id[df['CLS']])\n","df['label_id'] = lst"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'CLS'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-5e8a9690b88d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mlst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CLS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'CLS'"]}]},{"cell_type":"code","metadata":{"id":"Iyk5XqvBY6wf"},"source":["def compute_metrics(preds, labels):\n","    assert len(preds) == len(labels)\n","    return acc_and_f1(preds, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUEO0cYgBi7l"},"source":["def convert_inputs_to_features(\n","    texts,\n","    labels,\n","    max_seq_len,\n","    tokenizer,\n","    cls_token=\"[CLS]\",\n","    sep_token=\"[SEP]\",\n","    pad_token=0,\n","    mask_padding_with_zero=True,\n","):\n","    features = []\n","    for (ex_index, example) in enumerate(texts):\n","\n","        tokens_a = tokenizer.tokenize(example)\n","\n","        e11_p = tokens_a.index(\"<e1>\")  # the start position of entity1\n","        e12_p = tokens_a.index(\"</e1>\")  # the end position of entity1\n","        e21_p = tokens_a.index(\"<e2>\")  # the start position of entity2\n","        e22_p = tokens_a.index(\"</e2>\")  # the end position of entity2\n","\n","        # Replace the token\n","        tokens_a[e11_p] = \"$\"\n","        tokens_a[e12_p] = \"$\"\n","        tokens_a[e21_p] = \"#\"\n","        tokens_a[e22_p] = \"#\"\n","\n","        # Add 1 because of the [CLS] token\n","        e11_p += 1\n","        e12_p += 1\n","        e21_p += 1\n","        e22_p += 1\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = 2\n","        if len(tokens_a) > max_seq_len - special_tokens_count:\n","            tokens_a = tokens_a[: (max_seq_len - special_tokens_count)]\n","\n","        tokens = [cls_token] + tokens_a + [sep_token]\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n","        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_len - len(input_ids)\n","        input_ids = input_ids + ([pad_token] * padding_length)\n","        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n","\n","        # e1 mask, e2 mask\n","        e1_mask = [0] * len(attention_mask)\n","        e2_mask = [0] * len(attention_mask)\n","\n","        for i in range(e11_p, e12_p + 1):\n","            e1_mask[i] = 1\n","        for i in range(e21_p, e22_p + 1):\n","            e2_mask[i] = 1\n","\n","        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n","        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n","            len(attention_mask), max_seq_len\n","        )\n","        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(\n","            len(token_type_ids), max_seq_len\n","        )\n","\n","        label_id = int(labels)\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                label_id=label_id,\n","                e1_mask=e1_mask,\n","                e2_mask=e2_mask,\n","            )\n","        )\n","\n","    return features\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_rJ74tzQ8b2"},"source":["def load_and_cache_examples(tokenizer, mode):\n","    features = convert_examples_to_features(\n","            df_en.Sentence.tolist(), df_en.CLS.tolist(), 384, tokenizer, add_sep_token=True\n","        )\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n","    all_e1_mask = torch.tensor([f.e1_mask for f in features], dtype=torch.long)  # add e1 mask\n","    all_e2_mask = torch.tensor([f.e2_mask for f in features], dtype=torch.long)  # add e2 mask\n","\n","    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n","\n","    dataset = TensorDataset(\n","        all_input_ids,\n","        all_attention_mask,\n","        all_token_type_ids,\n","        all_label_ids,\n","        all_e1_mask,\n","        all_e2_mask,\n","    )\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9l5E6nfNSIBd"},"source":["class FCLayer(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n","        super(FCLayer, self).__init__()\n","        self.use_activation = use_activation\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        if self.use_activation:\n","            x = self.tanh(x)\n","        return self.linear(x)\n","\n","\n","class RBERT(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super(RBERT, self).__init__(config)\n","        self.bert = BertModel(config=config)  # Load pretrained bert\n","\n","        self.num_labels = config.num_labels\n","\n","        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, 0.1) # dropout_rate = 0.1\n","        self.entity_fc_layer = FCLayer(config.hidden_size, config.hidden_size, 0.1)\n","        self.label_classifier = FCLayer(\n","            config.hidden_size * 3,\n","            config.num_labels,\n","            0.1,\n","            use_activation=False,\n","        )\n","\n","    @staticmethod\n","    def entity_average(hidden_output, e_mask):\n","        \"\"\"\n","        Average the entity hidden state vectors (H_i ~ H_j)\n","        :param hidden_output: [batch_size, j-i+1, dim]\n","        :param e_mask: [batch_size, max_seq_len]\n","                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n","        :return: [batch_size, dim]\n","        \"\"\"\n","        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n","        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n","\n","        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n","        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n","        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n","        return avg_vector\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, labels, e1_mask, e2_mask):\n","        outputs = self.bert(\n","            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n","        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]  # [CLS]\n","\n","        # Average\n","        e1_h = self.entity_average(sequence_output, e1_mask)\n","        e2_h = self.entity_average(sequence_output, e2_mask)\n","\n","        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n","        pooled_output = self.cls_fc_layer(pooled_output)\n","        e1_h = self.entity_fc_layer(e1_h)\n","        e2_h = self.entity_fc_layer(e2_h)\n","\n","        # Concat -> fc_layer\n","        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n","        logits = self.label_classifier(concat_h)\n","\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        # Softmax\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsarWUbhU2v1"},"source":["class Trainer(object):\n","    def __init__(self, train_dataset=None, dev_dataset=None, test_dataset=None, num_train_epochs=10, weight_decay=0.0, adam_epsilon=1e-8, learning_rate=0.001, model_dir = './model'):\n","        #self.args = args\n","        self.train_dataset = train_dataset\n","        self.dev_dataset = dev_dataset\n","        self.test_dataset = test_dataset\n","\n","        self.label_lst = labels\n","        self.num_labels = len(self.label_lst)\n","\n","        self.config = BertConfig.from_pretrained(\n","            'bert-base-uncased',\n","            num_labels=self.num_labels,\n","            finetuning_task='semeval',\n","            id2label={str(i): label for i, label in enumerate(self.label_lst)},\n","            label2id={label: i for i, label in enumerate(self.label_lst)},\n","        )\n","        self.model = RBERT.from_pretrained('bert-base-uncased', config=self.config)\n","\n","        # GPU or CPU\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.model.to(self.device)\n","\n","    def train(self):\n","        train_sampler = RandomSampler(self.train_dataset)\n","        train_dataloader = DataLoader(\n","            self.train_dataset,\n","            sampler=train_sampler,\n","            batch_size=16,\n","        )\n","\n","        max_steps = -1\n","        gradient_accumulation_steps = 1\n","\n","        if max_steps > 0:\n","            t_total = max_steps\n","            self.num_train_epochs = (\n","                max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n","            )\n","        else:\n","            t_total = len(train_dataloader) // gradient_accumulation_steps * self.num_train_epochs\n","\n","        # Prepare optimizer and schedule (linear warmup and decay)\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=self.learning_rate,\n","            eps=self.adam_epsilon,\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=0,\n","            num_training_steps=t_total,\n","        )\n","\n","        # Train!\n","        print(\"***** Running training *****\")\n","        print(\"  Num examples = %d\", len(self.train_dataset))\n","        print(\"  Num Epochs = %d\", 10)\n","        print(\"  Total train batch size = %d\", 16)\n","        print(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n","        print(\"  Total optimization steps = %d\", t_total)\n","        #print(\"  Logging steps = %d\", self.args.logging_steps)\n","        print(\"  Save steps = %d\", 250)\n","\n","        global_step = 0\n","        tr_loss = 0.0\n","        self.model.zero_grad()\n","\n","        train_iterator = trange(int(16), desc=\"Epoch\")\n","\n","        for _ in train_iterator:\n","            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n","            for step, batch in enumerate(epoch_iterator):\n","                self.model.train()\n","                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n","                inputs = {\n","                    \"input_ids\": batch[0],\n","                    \"attention_mask\": batch[1],\n","                    \"token_type_ids\": batch[2],\n","                    \"labels\": batch[3],\n","                    \"e1_mask\": batch[4],\n","                    \"e2_mask\": batch[5],\n","                }\n","                outputs = self.model(**inputs)\n","                loss = outputs[0]\n","\n","                if gradient_accumulation_steps > 1:\n","                    loss = loss / gradient_accumulation_steps\n","\n","                loss.backward()\n","\n","                max_grad_norm = 1\n","\n","                tr_loss += loss.item()\n","                if (step + 1) % gradient_accumulation_steps == 0:\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n","\n","                    optimizer.step()\n","                    scheduler.step()  # Update learning rate schedule\n","                    self.model.zero_grad()\n","                    global_step += 1\n","\n","                    '''\n","                    if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n","                        self.evaluate(\"test\")  # There is no dev set for semeval task\n","\n","                    if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n","                        self.save_model()\n","                    '''\n","\n","                if 0 < max_steps < global_step:\n","                    epoch_iterator.close()\n","                    break\n","\n","            if 0 < max_steps < global_step:\n","                train_iterator.close()\n","                break\n","\n","        return global_step, tr_loss / global_step\n","\n","    def evaluate(self, mode):\n","        # We use test dataset because semeval doesn't have dev dataset\n","        if mode == \"test\":\n","            dataset = self.test_dataset\n","        elif mode == \"dev\":\n","            dataset = self.dev_dataset\n","        else:\n","            raise Exception(\"Only dev and test dataset available\")\n","\n","        eval_sampler = SequentialSampler(dataset)\n","        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=32)\n","\n","        # Eval!\n","        #print(\"***** Running evaluation on %s dataset *****\", mode)\n","        #print(\"  Num examples = %d\", len(dataset))\n","        #print(\"  Batch size = %d\", self.args.eval_batch_size)\n","        eval_loss = 0.0\n","        nb_eval_steps = 0\n","        preds = None\n","        out_label_ids = None\n","\n","        self.model.eval()\n","\n","        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","            batch = tuple(t.to(self.device) for t in batch)\n","            with torch.no_grad():\n","                inputs = {\n","                    \"input_ids\": batch[0],\n","                    \"attention_mask\": batch[1],\n","                    \"token_type_ids\": batch[2],\n","                    \"labels\": batch[3],\n","                    \"e1_mask\": batch[4],\n","                    \"e2_mask\": batch[5],\n","                }\n","                outputs = self.model(**inputs)\n","                tmp_eval_loss, logits = outputs[:2]\n","\n","                eval_loss += tmp_eval_loss.mean().item()\n","            nb_eval_steps += 1\n","\n","            if preds is None:\n","                preds = logits.detach().cpu().numpy()\n","                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","            else:\n","                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","        results = {\"loss\": eval_loss}\n","        preds = np.argmax(preds, axis=1)\n","        #write_prediction(self.args, os.path.join(self.args.eval_dir, \"proposed_answers.txt\"), preds)\n","\n","        result = compute_metrics(preds, out_label_ids)\n","        results.update(result)\n","\n","        for key in sorted(results.keys()):\n","            print(\"  {} = {:.4f}\".format(key, results[key]))\n","\n","        return results\n","\n","    '''\n","    def save_model(self):\n","        # Save model checkpoint (Overwrite)\n","        if not os.path.exists(self.model_dir):\n","            os.makedirs(self.model_dir)\n","        model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n","        model_to_save.save_pretrained(self.model_dir)\n","\n","        # Save training arguments together with the trained model\n","        torch.save(self.args, os.path.join(self.model_dir, \"training_args.bin\"))\n","        print(\"Saving model checkpoint to %s\", self.model_dir)\n","\n","    def load_model(self):\n","        # Check whether model exists\n","        if not os.path.exists(self.model_dir):\n","            raise Exception(\"Model doesn't exists! Train first!\")\n","\n","        self.args = torch.load(os.path.join(self.model_dir, \"training_args.bin\"))\n","        self.model = RBERT.from_pretrained(self.model_dir)\n","        self.model.to(self.device)\n","        print(\"***** Model Loaded *****\")\n","        '''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bes1VDTQY_3Y","colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"status":"error","timestamp":1638537550389,"user_tz":-330,"elapsed":2611,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"dd8febcd-3db0-4e9e-803d-9d1b13fe8362"},"source":["num_train_epochs=10\n","weight_decay=0.0\n","adam_epsilon=1e-8\n","learning_rate=0.001\n","\n","set_seed()\n","tokenizer = load_tokenizer()\n","\n","train_dataset = load_and_cache_examples(tokenizer, mode=\"train\")\n","test_dataset = load_and_cache_examples(tokenizer, mode=\"test\")\n","\n","trainer = Trainer(train_dataset, test_dataset, num_train_epochs, weight_decay, adam_epsilon, learning_rate)\n","\n","'''\n","if args.do_train:\n","    trainer.train()\n","\n","if args.do_eval:\n","    trainer.load_model()\n","    trainer.evaluate(\"test\")\n","'''"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-93dd6aefa491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-1df0391bb910>\u001b[0m in \u001b[0;36mload_and_cache_examples\u001b[0;34m(tokenizer, mode)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     features = convert_examples_to_features(\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0mdf_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_sep_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         )\n\u001b[1;32m      5\u001b[0m     \u001b[0mall_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-53ffba07e683>\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(texts, labels, max_seq_len, tokenizer, cls_token, cls_token_segment_id, sep_token, pad_token, pad_token_segment_id, sequence_a_segment_id, add_sep_token, mask_padding_with_zero)\u001b[0m\n\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mlabel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         features.append(\n","\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"]}]},{"cell_type":"code","metadata":{"id":"-kcdf1dPaMWY"},"source":[""],"execution_count":null,"outputs":[]}]}