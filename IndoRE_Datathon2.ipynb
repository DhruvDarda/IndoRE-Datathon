{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"IndoRE_Datathon2.ipynb","provenance":[{"file_id":"1NNWtSVNkl84I0-OtFsa3cKtPiDIhhG0J","timestamp":1638964841094},{"file_id":"1hH3GFnJ5HjXdWV9L7L7EEFLrHkOcVeuD","timestamp":1638869345202},{"file_id":"1KRH-1VOeWHAtuNHHzCY-gP5pQFuHraZZ","timestamp":1638624121527},{"file_id":"14NBfnuHPnRIWBNHm3LK-3FuXxZsD9reJ","timestamp":1635339660307},{"file_id":"15gyTrsd-OU6YZVyjwis48ysrXFPEcv9r","timestamp":1634569292161}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3539f61f-fe7d-4428-b074-327a883f7f6e","executionInfo":{"status":"ok","timestamp":1638961834792,"user_tz":-330,"elapsed":4539,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"cbf3040a-2245-4a2a-ad26-62d6b284c2cb"},"source":["!pip install transformers\n","import json\n","import copy\n","import os\n","import math\n","import torch\n","import torch.nn as nn\n","from torchtext.legacy import data\n","import random\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from torch import Tensor \n","import pandas as pd\n","import torch.nn.functional as F\n","from transformers import (WEIGHTS_NAME, BertConfig, BertModel, BertPreTrainedModel, BertTokenizer)\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler"],"id":"3539f61f-fe7d-4428-b074-327a883f7f6e","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Pj321J54D9O","executionInfo":{"status":"ok","timestamp":1638961745761,"user_tz":-330,"elapsed":6233,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"fc8f7385-cd3b-410c-e229-d80a2e4da4a6"},"source":["import pandas as pd\n","#from torchtext.data.metrics import bleu_score\n","from sklearn.model_selection import train_test_split\n","#from torch.utils.tensorboard import SummaryWriter\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","%cd /content/gdrive/My Drive/Kaggle\n"],"id":"2Pj321J54D9O","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","/content/gdrive/My Drive/Kaggle\n"]}]},{"cell_type":"code","metadata":{"id":"zIb4mb4cy2jJ"},"source":["ADDITIONAL_SPECIAL_TOKENS = ['<e1>', \"</e1>\", \"<e2>\", \"</e2>\"]\n","def load_tokenizer():\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n","    return tokenizer"],"id":"zIb4mb4cy2jJ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMZA3j7ejTGU"},"source":["class InputFeatures(object):\n","    \"\"\"\n","    A single set of features of data.\n","    Args:\n","        input_ids: Indices of input sequence tokens in the vocabulary.\n","        attention_mask: Mask to avoid performing attention on padding token indices.\n","            Mask values selected in ``[0, 1]``:\n","            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n","        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n","    \"\"\"\n","\n","    def __init__(self, input_ids, attention_mask, label_id, e1_mask, e2_mask):\n","        self.input_ids = input_ids\n","        self.attention_mask = attention_mask\n","        self.label_id = label_id\n","        self.e1_mask = e1_mask\n","        self.e2_mask = e2_mask\n","\n","    def __repr__(self):\n","        return str(self.to_json_string())\n","\n","    def to_dict(self):\n","        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n","        output = copy.deepcopy(self.__dict__)\n","        return output\n","\n","    def to_json_string(self):\n","        \"\"\"Serializes this instance to a JSON string.\"\"\"\n","        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""],"id":"pMZA3j7ejTGU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEttiL1by-Lh"},"source":["def convert_inputs_to_features(\n","    texts,\n","    label_id,\n","    max_seq_len,\n","    tokenizer,\n","    cls_token=\"[CLS]\",\n","    sep_token=\"[SEP]\",\n","    pad_token=0,\n","    mask_padding_with_zero=True,\n","):\n","    features = []\n","    #print(texts)\n","    for (ex_index, example) in enumerate(texts):\n","\n","        #print(ex_index, example)\n","        tokens_a = tokenizer.tokenize(example)\n","        #print(tokens_a)\n","\n","        e11_p = tokens_a.index(\"<e1>\")  # the start position of entity1\n","        e12_p = tokens_a.index(\"</e1>\")  # the end position of entity1\n","        e21_p = tokens_a.index(\"<e2>\")  # the start position of entity2\n","        e22_p = tokens_a.index(\"</e2>\")  # the end position of entity2\n","\n","        # Replace the token\n","        tokens_a[e11_p] = \"$\"\n","        tokens_a[e12_p] = \"$\"\n","        tokens_a[e21_p] = \"#\"\n","        tokens_a[e22_p] = \"#\"\n","\n","        # Add 1 because of the [CLS] token\n","        e11_p += 1\n","        e12_p += 1\n","        e21_p += 1\n","        e22_p += 1\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = 2\n","        if len(tokens_a) > max_seq_len - special_tokens_count:\n","            tokens_a = tokens_a[: (max_seq_len - special_tokens_count)]\n","\n","        tokens = [cls_token] + tokens_a + [sep_token]\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n","        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_len - len(input_ids)\n","        input_ids = input_ids + ([pad_token] * padding_length)\n","        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n","\n","        # e1 mask, e2 mask\n","        e1_mask = [0] * len(attention_mask)\n","        e2_mask = [0] * len(attention_mask)\n","\n","        for i in range(e11_p, e12_p + 1):\n","            e1_mask[i] = 1\n","        for i in range(e21_p, e22_p + 1):\n","            e2_mask[i] = 1\n","\n","        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n","        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n","            len(attention_mask), max_seq_len\n","        )\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                label_id=label_id,\n","                e1_mask=e1_mask,\n","                e2_mask=e2_mask,\n","            )\n","        )\n","\n","    return features\n"],"id":"QEttiL1by-Lh","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zx-O0kkA3JKH","executionInfo":{"status":"ok","timestamp":1638961745771,"user_tz":-330,"elapsed":30,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"fc55d362-7470-47d6-81d2-fdb4ce12cee5"},"source":["'''\n","#!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n","#!pip install inltk\n","\n","from inltk.inltk import identify_language, tokenize, setup_language, reset_language_identifying_models\n","reset_language_identifying_models()\n","#setup_language('hi')\n","#setup_language('bn')\n","#setup_language('te')\n","#identify_language('hello, its english')\n","'''\n","\n","#df = pd.read_csv('train.tsv', sep='\\t')\n","df_en = pd.read_csv('en.tsv', sep='\\t')\n","#df = pd.concat([df, df_en])\n","\n","#inputs = []\n","'''\n","lang = identify_language(df['Sentence'].iloc[1]).run_until_complete()\n","\n","for i in range(len(df)):\n","  lang = identify_language(df['Sentence'].iloc[i])\n","  sentence = ' '.join(tokenize(df['Sentence'].iloc[i], lang))\n","  inputs.append(df['NER1'].iloc[i]+'-'+df['NER2'].iloc[i]+' '+sentence)\n","'''\n","'''\n","for i in range(len(df)):\n","  inputs.append(df['NER1'].iloc[i]+'-'+df['NER2'].iloc[i]+' '+df['Sentence'].iloc[i])\n","df['CLS_sentence'] = inputs\n","\n","df = df[['CLS_sentence', 'Relation']]\n","'''\n","labels = df_en['Relation'].unique().tolist()\n","labels_map = {}\n","for i in range(len(labels)):\n","  labels_map[labels[i]] = i\n","\n","ids = []\n","\n","for i in range(len(df_en)):\n","  ids.append(labels_map[df_en['Relation'].iloc[i]])\n","df_en['Label_id'] = ids\n","\n","df = df_en[['Sentence', 'Label_id']]\n","\n","df = df.sample(frac=1).reset_index(drop=True)\n","\n","#print(df['CLS_sentence'].iloc[0])\n","              \n","#train, test = train_test_split(df, test_size = 0.2, shuffle = True, random_state = 42)\n","\n","print()"],"id":"Zx-O0kkA3JKH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"rUDr6dA1k2av"},"source":["def Load_Dataset():\n","    tokenizer = load_tokenizer()\n","    features = convert_inputs_to_features(df['Sentence'].tolist(), df['Label_id'].tolist(), 120, tokenizer)\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","    all_e1_mask = torch.tensor([f.e1_mask for f in features], dtype=torch.long)  # add e1 mask\n","    all_e2_mask = torch.tensor([f.e2_mask for f in features], dtype=torch.long)  # add e2 mask\n","\n","    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n","\n","    dataset = TensorDataset(\n","        all_input_ids,\n","        all_attention_mask,\n","        all_label_ids,\n","        all_e1_mask,\n","        all_e2_mask\n","    )\n","    return dataset"],"id":"rUDr6dA1k2av","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"edMq86hzlQZg"},"source":["train_dataset = Load_Dataset()"],"id":"edMq86hzlQZg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["PAD_TOKEN, PAD_INDEX = '[PAD]', 0\n","UNK_TOKEN, UNK_INDEX = '[UNK]', 1\n","MASK_TOKEN, MASK_INDEX = '[MASK]', 2\n","CLS_TOKEN, CLS_INDEX = '[CLS]', 3\n","SEP_TOKEN, SEP_INDEX = '[SEP]', 4"],"metadata":{"id":"NrT5iIHh8oPi"},"id":"NrT5iIHh8oPi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEmbedding(nn.Module):\n","\n","    def __init__(self, max_len, hidden_size, ):\n","        super(PositionalEmbedding, self).__init__()\n","        self.positional_embedding = nn.Embedding(max_len, hidden_size)\n","        positions = torch.arange(0, max_len)\n","        self.register_buffer('positions', positions)\n","\n","    def forward(self, sequence):\n","        batch_size, seq_len = sequence.size()\n","        positions = self.positions[:seq_len].unsqueeze(0).repeat(batch_size, 1)\n","        return self.positional_embedding(positions)"],"metadata":{"id":"T6qwbRtm4UVe"},"id":"T6qwbRtm4UVe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GELU(nn.Module):\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","class TransformerEncoder(nn.Module):\n","\n","    def __init__(self, layers_count, d_model, heads_count, d_ff, dropout_prob):\n","        super(TransformerEncoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.encoder_layers = nn.ModuleList(\n","            [TransformerEncoderLayer(d_model, heads_count, d_ff, dropout_prob) for _ in range(layers_count)]\n","        )\n","\n","    def forward(self, sources, mask):\n","        \"\"\"Transformer bidirectional encoder\n","        args:\n","           sources: embedded_sequence, (batch_size, seq_len, embed_size)\n","        \"\"\"\n","        for encoder_layer in self.encoder_layers:\n","            sources = encoder_layer(sources, mask)\n","\n","        return sources\n","\n","\n","class TransformerEncoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, heads_count, d_ff, dropout_prob):\n","        super(TransformerEncoderLayer, self).__init__()\n","\n","        self.self_attention_layer = Sublayer(MultiHeadAttention(heads_count, d_model, dropout_prob), d_model)\n","        self.pointwise_feedforward_layer = Sublayer(PointwiseFeedForwardNetwork(d_ff, d_model, dropout_prob), d_model)\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, sources, sources_mask):\n","        # x: (batch_size, seq_len, d_model)\n","\n","        sources = self.self_attention_layer(sources, sources, sources, sources_mask)\n","        sources = self.dropout(sources)\n","        sources = self.pointwise_feedforward_layer(sources)\n","\n","        return sources\n","\n","\n","class Sublayer(nn.Module):\n","\n","    def __init__(self, sublayer, d_model):\n","        super(Sublayer, self).__init__()\n","\n","        self.sublayer = sublayer\n","        self.layer_normalization = LayerNormalization(d_model)\n","\n","    def forward(self, *args):\n","        x = args[0]\n","        x = self.sublayer(*args) + x\n","        return self.layer_normalization(x)\n","\n","\n","class LayerNormalization(nn.Module):\n","\n","    def __init__(self, features_count, epsilon=1e-6):\n","        super(LayerNormalization, self).__init__()\n","\n","        self.gain = nn.Parameter(torch.ones(features_count))\n","        self.bias = nn.Parameter(torch.zeros(features_count))\n","        self.epsilon = epsilon\n","\n","    def forward(self, x):\n","\n","        mean = x.mean(dim=-1, keepdim=True)\n","        std = x.std(dim=-1, keepdim=True)\n","\n","        return self.gain * (x - mean) / (std + self.epsilon) + self.bias\n","\n","\n","class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, heads_count, d_model, dropout_prob, mode='self-attention'):\n","        super(MultiHeadAttention, self).__init__()\n","\n","        assert d_model % heads_count == 0\n","        assert mode in ('self-attention', 'memory-attention')\n","\n","        self.d_head = d_model // heads_count\n","        self.heads_count = heads_count\n","        self.mode = mode\n","        self.query_projection = nn.Linear(d_model, heads_count * self.d_head)\n","        self.key_projection = nn.Linear(d_model, heads_count * self.d_head)\n","        self.value_projection = nn.Linear(d_model, heads_count * self.d_head)\n","        self.final_projection = nn.Linear(d_model, heads_count * self.d_head)\n","        self.dropout = nn.Dropout(dropout_prob)\n","        self.softmax = nn.Softmax(dim=3)\n","\n","        self.attention = None\n","        # For cache\n","        self.key_projected = None\n","        self.value_projected = None\n","\n","    def forward(self, query, key, value, mask=None, layer_cache=None):\n","        \"\"\"\n","        Args:\n","            query: (batch_size, query_len, model_dim)\n","            key: (batch_size, key_len, model_dim)\n","            value: (batch_size, value_len, model_dim)\n","            mask: (batch_size, query_len, key_len)\n","        \"\"\"\n","        # print('attention mask', mask)\n","        batch_size, query_len, d_model = query.size()\n","\n","        d_head = d_model // self.heads_count\n","\n","        query_projected = self.query_projection(query)\n","        # print('query_projected', query_projected.shape)\n","        if layer_cache is None or layer_cache[self.mode] is None:  # Don't use cache\n","            key_projected = self.key_projection(key)\n","            value_projected = self.value_projection(value)\n","        else:  # Use cache\n","            if self.mode == 'self-attention':\n","                key_projected = self.key_projection(key)\n","                value_projected = self.value_projection(value)\n","\n","                key_projected = torch.cat([key_projected, layer_cache[self.mode]['key_projected']], dim=1)\n","                value_projected = torch.cat([value_projected, layer_cache[self.mode]['value_projected']], dim=1)\n","            elif self.mode == 'memory-attention':\n","                key_projected = layer_cache[self.mode]['key_projected']\n","                value_projected = layer_cache[self.mode]['value_projected']\n","\n","        # For cache\n","        self.key_projected = key_projected\n","        self.value_projected = value_projected\n","\n","        batch_size, key_len, d_model = key_projected.size()\n","        batch_size, value_len, d_model = value_projected.size()\n","\n","        query_heads = query_projected.view(batch_size, query_len, self.heads_count, d_head).transpose(1, 2)  # (batch_size, heads_count, query_len, d_head)\n","        key_heads = key_projected.view(batch_size, key_len, self.heads_count, d_head).transpose(1, 2)  # (batch_size, heads_count, key_len, d_head)\n","        value_heads = value_projected.view(batch_size, value_len, self.heads_count, d_head).transpose(1, 2)  # (batch_size, heads_count, value_len, d_head)\n","\n","        attention_weights = self.scaled_dot_product(query_heads, key_heads)  # (batch_size, heads_count, query_len, key_len)\n","\n","        if mask is not None:\n","            mask_expanded = mask.unsqueeze(1).expand_as(attention_weights)\n","            attention_weights = attention_weights.masked_fill(mask_expanded, -1e18)\n","\n","        self.attention = self.softmax(attention_weights)  # Save attention to the object\n","        attention_dropped = self.dropout(self.attention)\n","        context_heads = torch.matmul(attention_dropped, value_heads)  # (batch_size, heads_count, query_len, d_head)\n","        context_sequence = context_heads.transpose(1, 2).contiguous()  # (batch_size, query_len, heads_count, d_head)\n","        context = context_sequence.view(batch_size, query_len, d_model)  # (batch_size, query_len, d_model)\n","        final_output = self.final_projection(context)\n","\n","        return final_output\n","\n","    def scaled_dot_product(self, query_heads, key_heads):\n","        \"\"\"\n","        Args:\n","             query_heads: (batch_size, heads_count, query_len, d_head)\n","             key_heads: (batch_size, heads_count, key_len, d_head)\n","        \"\"\"\n","        key_heads_transposed = key_heads.transpose(2, 3)\n","        dot_product = torch.matmul(query_heads, key_heads_transposed)  # (batch_size, heads_count, query_len, key_len)\n","        attention_weights = dot_product / np.sqrt(self.d_head)\n","        return attention_weights\n","\n","\n","class PointwiseFeedForwardNetwork(nn.Module):\n","\n","    def __init__(self, d_ff, d_model, dropout_prob):\n","        super(PointwiseFeedForwardNetwork, self).__init__()\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.Dropout(dropout_prob),\n","            GELU(),\n","            nn.Linear(d_ff, d_model),\n","            nn.Dropout(dropout_prob),\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","             x: (batch_size, seq_len, d_model)\n","        \"\"\"\n","        return self.feed_forward(x)"],"metadata":{"id":"0Ha0VOCd4VmA"},"id":"0Ha0VOCd4VmA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad_masking(x):\n","    # x: (batch_size, seq_len)\n","    padded_positions = x == PAD_INDEX\n","    return padded_positions.unsqueeze(1)"],"metadata":{"id":"2LT3KnTm8TG2"},"id":"2LT3KnTm8TG2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERT(nn.Module):\n","\n","    def __init__(self, encoder, token_embedding, positional_embedding, hidden_size, vocabulary_size, num_lables):\n","        super(BERT, self).__init__()\n","\n","        self.encoder = encoder\n","        self.token_embedding = token_embedding\n","        self.positional_embedding = positional_embedding\n","        self.token_prediction_layer = nn.Linear(hidden_size, vocabulary_size)\n","        self.classification_layer = nn.Linear(hidden_size, num_labels)\n","\n","    def forward(self, inputs):\n","        sequence, segment = inputs\n","        token_embedded = self.token_embedding(sequence)\n","        positional_embedded = self.positional_embedding(sequence)\n","        embedded_sources = token_embedded + positional_embedded\n","\n","        mask = pad_masking(sequence)\n","        encoded_sources = self.encoder(embedded_sources, mask)\n","        token_predictions = self.token_prediction_layer(encoded_sources)\n","        classification_embedding = encoded_sources[:, 0, :]\n","        classification_output = self.classification_layer(classification_embedding)\n","        return token_predictions, classification_output"],"metadata":{"id":"XsenU3LF4FWi"},"id":"XsenU3LF4FWi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model(layers_count, hidden_size, heads_count, d_ff, dropout_prob, max_len, vocabulary_size):\n","    token_embedding = nn.Embedding(num_embeddings=vocabulary_size, embedding_dim=hidden_size)\n","    positional_embedding = PositionalEmbedding(max_len=max_len, hidden_size=hidden_size)\n","    segment_embedding = SegmentEmbedding(hidden_size=hidden_size)\n","\n","    encoder = TransformerEncoder(\n","        layers_count=layers_count,\n","        d_model=hidden_size,\n","        heads_count=heads_count,\n","        d_ff=d_ff,\n","        dropout_prob=dropout_prob)\n","\n","    bert = BERT(\n","        encoder=encoder,\n","        token_embedding=token_embedding,\n","        positional_embedding=positional_embedding,\n","        segment_embedding=segment_embedding,\n","        hidden_size=hidden_size,\n","        vocabulary_size=vocabulary_size)\n","\n","    return bert"],"metadata":{"id":"cm8OQyKR34dH"},"id":"cm8OQyKR34dH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FnFTvUudDL9W"},"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# device=\"cpu\""],"id":"FnFTvUudDL9W","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXQwsyWvpdn1"},"source":["#%%writefile app.py\n","def save_checkpoint(state, filename=\"my_check.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(PATH, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(PATH)\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    epoch = checkpoint['epoch']\n","    return epoch, model, optimizer"],"id":"UXQwsyWvpdn1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)"],"metadata":{"id":"c1N4_VVoJG4f"},"id":"c1N4_VVoJG4f","execution_count":null,"outputs":[]},{"cell_type":"code","source":["adam_epsilon=1e-8\n","warmup_steps=0\n","max_steps = -1\n","learning_rate = 3e-5\n","\n","train_sampler = RandomSampler(train_dataset)\n","\n","train_dataloader = DataLoader(\n","        train_dataset, sampler=train_sampler, batch_size= train_batch_size)\n","\n","if max_steps > 0:\n","    t_total = max_steps\n","    num_train_epochs = max_steps // len(train_dataloader) + 1\n","else:\n","    t_total = len(train_dataloader) // num_train_epochs\n","        \n","# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters()\n","                    if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-3},\n","        {'params': [p for n, p in model.named_parameters()\n","                    if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","optimizer = AdamW(optimizer_grouped_parameters,\n","                      lr=learning_rate, eps=adam_epsilon)\n","scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"jfPbc9u3JsQf","executionInfo":{"status":"error","timestamp":1638961900223,"user_tz":-330,"elapsed":451,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"e0db68c7-ab53-43c5-98b2-575a859a1716"},"id":"jfPbc9u3JsQf","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-91fed184e096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m train_dataloader = DataLoader(\n\u001b[0m\u001b[1;32m      9\u001b[0m         train_dataset, sampler=train_sampler, batch_size= train_batch_size)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"]}]},{"cell_type":"code","source":["def train(train_dataloader, optimizer, scheduler, train_dataset, model, tokenizer, num_train_epochs=5, max_grad_norm = 1.0):\n","    \"\"\" Train the model \"\"\"\n","    train_dataloader = train_dataloader\n","    optimizer = optimizer\n","    scheduler = scheduler\n","\n","    global_step = 0\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    train_iterator = trange(int(num_train_epochs), desc=\"Epoch\")\n","\n","    # Added here for reproductibility (even between python 2 and 3)\n","    set_seed(42)\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            batch = tuple(t.to(device) for t in batch)\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      # XLM and RoBERTa don't use segment_ids\n","                      'token_type_ids': batch[2],\n","                      'labels':      batch[3],\n","                      'e1_mask': batch[4],\n","                      'e2_mask': batch[5],\n","                      }\n","\n","            outputs = model(**inputs)\n","            # model outputs are always tuple in pytorch-transformers (see doc)\n","            loss = outputs[0]\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                model.parameters(), max_grad_norm)\n","\n","            tr_loss += loss.item()\n","\n","            optimizer.step()\n","            scheduler.step()  # Update learning rate schedule\n","            model.zero_grad()\n","            global_step += 1\n","\n","            if max_steps > 0 and global_step > max_steps:\n","                epoch_iterator.close()\n","                break\n","        if max_steps > 0 and global_step > max_steps:\n","            train_iterator.close()\n","            break\n","    return global_step, tr_loss / global_step"],"metadata":{"id":"KkM6V0_Rrte5"},"id":"KkM6V0_Rrte5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["bertconfig = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=bertconfig)\n","\n","train(train_dataloader, train_dataloader, optimizer, scheduler, train_dataset, model, tokenizer, num_train_epochs=5, max_grad_norm = 1.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"qkYrQQ4stEhd","executionInfo":{"status":"error","timestamp":1638962500318,"user_tz":-330,"elapsed":1169,"user":{"displayName":"Dhruv Darda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh55iWc2iuyxE_piHnST2n-NvILAQilQvZqbwWn4g=s64","userId":"05620612540023835884"}},"outputId":"851fc184-a1a5-48f9-ba8a-54b8be13c00c"},"id":"qkYrQQ4stEhd","execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-801525a4c9f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbertconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbertconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mno_init_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_enable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fast_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfrom_pt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-a7b151eec289>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-large-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dropout on CLS transformed token embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dropout on average entity embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_pooling_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             raise ValueError(\n\u001b[0;32m--> 486\u001b[0;31m                 \u001b[0;34mf\"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"`PretrainedConfig`. To create a model from a pretrained model use \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0;34mf\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Parameter config in `BertModel(config)` should be an instance of class `PretrainedConfig`. To create a model from a pretrained model use `model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)`"]}]},{"cell_type":"code","metadata":{"id":"fSo606ffNEn7"},"source":["# %%writefile app.py\n","path = %pwd\n","PATH = os.path.join(path, 'my_check.pth.tar')\n","epoch, model, opt = load_checkpoint(PATH, model, opt)\n","df = pd.read_csv('valid.tsv', sep='\\t')\n","\n","outputs = []\n","for i in range(len(df)):\n","  outputs.append(predict(model, get_inp_sen_tensor(df['Sentence'].iloc[i]))[:-5])\n","\n","df['Relation'] = outputs\n","df['Relation'].to_csv('submission.csv')"],"id":"fSo606ffNEn7","execution_count":null,"outputs":[]}]}